## \_ translate

#### Just started bro.

I hope I dont give up mid way

---

- [x] Full Transformer as by Vaswani et al.

  - First step is try to understand and replicate Transformer model. Take some notes along the way.

- [x] FlashAttention

  - Improve it by using FlashAttention and rotational embedding taking nanoGPT as a reference.-

- [ ] rotational positional embedding

- [ ] Group Query Attention
